{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6a25c-eb88-4523-9975-6114ce3919cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"./szurkemarha-6.5k\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NYTK/PULI-GPT-3SX\", return_dict=True, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/PULI-GPT-3SX\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id, device_map={'':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb877f-7c11-4f79-8628-31ade5b464f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=.4,\n",
    ")\n",
    "prompt = 'Alább található egy instrukció, ami egy feladatot ír le, valamint egy bemenet, ami további kontextust ad. Írj egy illő választ, ami helyesen teljesíti a kérést!\\n\\n'\n",
    "prompt_no_input = 'Alább található egy instrukció, ami egy feladatot ír le. Írj egy illő választ, ami helyesen teljesíti a kérést!\\n\\n'\n",
    "data = {'instruction': 'Ajánlj egy sci-fi könyvet!', 'input': None}\n",
    "#data = {'instruction': 'Add meg a vásárló nevét!', 'input': 'Péter tegnap vett egy autót Petrától.'}\n",
    "text = (prompt if data['input'] is not None else prompt_no_input)+'### Instrukció:\\n' + data['instruction'].strip() + ('\\n\\n### Bemenet:\\n'+data['input'].strip() if data['input'] is not None else '') + '\\n\\n### Válasz:'\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].cuda()\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(input_ids=input_ids, do_sample=True, max_new_tokens=64, pad_token_id=tokenizer.pad_token_id, generation_config=generation_config)\n",
    "    result = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
